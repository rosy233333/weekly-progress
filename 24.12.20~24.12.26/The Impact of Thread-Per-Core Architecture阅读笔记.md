# The Impact of Thread-Per-Core Architecture on  Application Tail Latency阅读笔记

时间：2024/12/22

## 我从中学到的

1. 实现高性能的thread-per-core结构，需要消除线程阻塞（改用非阻塞接口），并尽量减少线程同步。
2. thread-per-core结构与不同资源管理方式的结合。
3. 对于需要请求较为耗时的OS资源（例如，接收网络请求）的应用，使用户线程和内核线程分别占据一部分CPU核心、减少特权级切换的设计能取得更好的性能。此时可以看作用户态和内核态整体的thread-per-core结构。

## Abstract

thread-per-core结构（下文简称TPC）可以降低切换开销，但也会面临线程同步、OS接口上的挑战。（？）

时延表明，与基准相比，可以降低71%的开销。

## Introduction

thread-per-core结构的优势：降低CPU用于同步和多路复用（大概就是上下文切换？）开销、允许线程在粗粒度下调度和均衡。

thread-per-core结构的不足：只在各线程可以独立运行的情况下可以取得性能优势，而**线程同步**和**阻塞式OS服务**使线程无法独立运行。但这些可以被规避：前者通过数据分区减少同步，后者通过强制使用异步OS服务从而消除阻塞。

本文通过以下几个方面探究TPC结构对尾部延迟的影响：

- 探索TPC结构的设计空间（shared-everything、shared-nothing、shared-something）和中断处理方式（如中断亲和度和平衡）的影响。
- 在不同的设计和中断处理方式的组合中，测量TPC结构对应用尾部时延的影响。
- 讨论TPC结构如何被请求转向开销、OS抽象和接口阻碍，且应用程序如何利用硬件卸载。

## Background

CPU亲和度问题：**一个数据包在接收过程中最多可以经过3个CPU核心**：首先放入与核心对应的RX队列中，再由另一个运行内核网络栈的核心处理（需要启用内核软件转向），最后内核将数据包传递给另一个运行用户程序的核心。

线程同步：在多个线程访问同一个请求时产生。

OS接口：OS接口带来的开销主要在系统调用产生的特权级切换、数据的复制（内核空间到用户空间）、安全机制的实现上。为了降低开销而使用I/O多路复用系统调用。但其只是提供了通知功能，实际I/O功能还需要进行一次系统调用。并且，通知机制可能会带来核间中断。

## Thread-Per-Core Architecture

TPC结构：为每个CPU核心设置一个线程，且该线程会固定在相应的CPU核心上。

### 资源管理

![](../图片/屏幕截图%202024-12-24%20144647.png)

**Shared-everything**：为了在TPC结构中使用shared-everything，应用需要采用非阻塞的数据同步方式。

- 优点：所有CPU都可用于处理请求带来的吞吐量提升（**我的理解：与部分共享、不共享不同，完全共享不会遇到因为数据局部性导致的，同一时间只有部分核心可以处理请求的情况，也就是任何时候都可以使用所有核心处理请求。**）
- 缺点：数据在多个CPU间传递导致的缓存失效、线程同步开销（即使是非阻塞方式也有开销）

**Shared-nothing**：每个线程只能访问分配给它的部分数据。完全消除了同步开销，但可能需要引入线程间通信。（有时候，数据分区+线程间通信的开销低于共享内存+锁）

- 优点：完全消除了同步开销，并且提高了缓存命中率
- 缺点：对于不均衡的数据，可能只有几个CPU可以工作，导致吞吐量的下降。

**Shared-something**：两种方法的折中。

### 中断亲和度

Linux的中断平衡机制会将中断分配到CPU核心上。不过，在特定几个核心上处理中断，可以避免运行用户程序的核心被中断。该设计可以通过设置中断亲和度（哪些核心可以处理给定的中断）做到。

在特定的CPU核心上运行网络栈、处理网络中断的优势：

1. 数据包处理更有效
2. 减少线程被打断的次数

## Impact on Tail Latency

比较对象：已有的最优实现Memcached和自己实现的Sphinx，为键值对数据库。

Memcached：通过将线程池大小设定为CPU核数，从而模拟shared-everything的TPC结构。

Sphinx：如下图所示，（1）使用不同的CPU核心将OS资源和应用数据分离，（2）线程间消息传递

![](../图片/屏幕截图%202024-12-24%20153516.png)

### Sphinx的设计

**数据分区**：不仅包括应用数据（数据库的键），还包括OS资源（socket、TCP连接），将这些资源按照CPU核心分配。

**线程间消息传递**：如果接收到请求的核心不具有请求需要处理的键，则通过向拥有该键的核心发送消息，其处理完成后再向原核心发送消息，原核心从而响应请求。该设计的目的是为了确保每个socket只被一个核心访问。

### 实验配置

硬件配置：高端配置（网卡的RX和TX队列数量与核心数相等）、中端配置（网卡的RX队列数量小于核心数）

中断设置：

- 不配置中断亲和度、启用中断均衡（作为基准，因为其是大部分系统的默认配置）
- 不配置中断亲和度、禁用中断均衡（分离出中断均衡对尾部延迟的影响）
- 配置中断亲和度、禁用中断均衡（使用一部分核心处理中断、一部分核心运行用户程序。禁用中断均衡是因为其会覆盖中断亲和度的设置）

### 实验结果

![](../图片/屏幕截图%202024-12-24%20161316.png)

1. 在三种中断设置中，配置中断亲和度、禁用中断均衡的选项具有最低的尾部时延。
2. 对于更新操作，Sphinx的尾部时延总是低于Memcached；而对于读操作，Sphinx的尾部时延只在以下条件成立时明显低于Memcached：（1）配置中断亲和度、禁用中断均衡，且（2）并发度较大。
3. 中断均衡对尾部时延的影响明显小于中断亲和度。
4. 当并发度较小时，Sphinx的尾部时延高于Memcached。这说明在工作负载无法均衡到每个核心上时，TPC结构会显示出弱点。
5. 设置中断亲和度对提升TPC结构的性能很有效。
